<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Computational Cognitive Neuroscience - Something to Live</title><meta name=Description content="Tech Notes of Wen-Wei Tseng"><meta property="og:title" content="Computational Cognitive Neuroscience"><meta property="og:description" content="Course Notes of Computational Cognitive Neuroscience by Prof. 鄭士康."><meta property="og:type" content="article"><meta property="og:url" content="https://sosiristseng.github.io/posts/course/ccns/"><meta property="og:image" content="https://sosiristseng.github.io/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-18T17:29:03+08:00"><meta property="article:modified_time" content="2021-06-18T17:44:45+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sosiristseng.github.io/"><meta name=twitter:title content="Computational Cognitive Neuroscience"><meta name=twitter:description content="Course Notes of Computational Cognitive Neuroscience by Prof. 鄭士康."><meta name=application-name content="Los los los"><meta name=apple-mobile-web-app-title content="Los los los"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=parrot.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://sosiristseng.github.io/posts/course/ccns/><link rel=prev href=https://sosiristseng.github.io/posts/course/academic-writing-04/><link rel=next href=https://sosiristseng.github.io/posts/course/circuits/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload="this.onload=null,this.rel='stylesheet'" href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css></noscript><link rel=preload as=style onload="this.onload=null,this.rel='stylesheet'" href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Computational Cognitive Neuroscience","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/sosiristseng.github.io\/posts\/course\/ccns\/"},"genre":"posts","wordcount":4121,"url":"https:\/\/sosiristseng.github.io\/posts\/course\/ccns\/","datePublished":"2021-06-18T17:29:03+08:00","dateModified":"2021-06-18T17:44:45+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx"},"author":{"@type":"Person","name":"Wen-Wei Tseng"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(a){document.body.setAttribute('theme',a)}function saveTheme(a){window.localStorage&&localStorage.setItem('theme',a)}function getMeta(b){const a=document.getElementsByTagName('meta');for(let c=0;c<a.length;c++)if(a[c].getAttribute('name')===b)return a[c];return''}if(window.localStorage&&localStorage.getItem('theme')){let a=localStorage.getItem('theme');a==='light'||a==='dark'||a==='black'?setTheme(a):window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?setTheme('dark'):setTheme('light')}else'auto'==='light'||'auto'==='dark'||'auto'==='black'?(setTheme('auto'),saveTheme('auto')):(saveTheme('auto'),window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?setTheme('dark'):setTheme('light'));let themeColorMeta=getMeta('theme-color');document.body.getAttribute('theme')!='light'&&(themeColorMeta.content='#000000')</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Something to Live"><span class=header-title-pre><i class="fas fa-kiwi-bird fa-fw"></i></span>Something to Live</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/ title=Posts>Posts </a><a class=menu-item href=/tags/ title=Tags>Tags </a><a class=menu-item href=/categories/ title=Categories>Categories </a><a class=menu-item href=/about/ title="About Me">About </a><a class=menu-item href=https://github.com/sosiristseng/ title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github-alt fa-fw"></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop><input type=text placeholder=Search id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=# onclick=return!1 class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Something to Live"><span class=header-title-pre><i class="fas fa-kiwi-bird fa-fw"></i></span>Something to Live</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title=Posts>Posts</a><a class=menu-item href=/tags/ title=Tags>Tags</a><a class=menu-item href=/categories/ title=Categories>Categories</a><a class=menu-item href=/about/ title="About Me">About</a><a class=menu-item href=https://github.com/sosiristseng/ title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github-alt fa-fw"></i></a><a href=# onclick=return!1 class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><script>document.body.setAttribute("pageStyle","wide")</script><script>document.body.setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Computational Cognitive Neuroscience</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://sosiristseng.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="author fas fa-user-circle fa-fw"></i>Wen-Wei Tseng</a>
</span>&nbsp;<span class=post-category>included in <a href=/categories/course-notes/><i class="far fa-folder fa-fw"></i>Course Notes</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-06-18>2021-06-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;4121 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;9 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#course-information>Course Information</a></li><li><a href=#central-questions>Central Questions</a><ul><li><a href=#cognitive-psychology>Cognitive Psychology</a></li><li><a href=#artificial-intelligence>Artificial intelligence</a></li><li><a href=#biological-plausibility>Biological plausibility</a></li><li><a href=#levels-scales-of-nervous-system>Levels (scales) of nervous system</a></li></ul></li><li><a href=#builidng-a-brain-with-math-models>Builidng a brain with math models</a><ul><li><a href=#3d-brain-structure>3D brain structure</a></li><li><a href=#the-scale-of-brain-models>The scale of brain models</a></li><li><a href=#neuron-biology>Neuron biology</a></li><li><a href=#hodgkin-and-huxley-model-1952>Hodgkin and Huxley model (1952)</a><ul><li><a href=#derived-models>Derived models</a></li></ul></li><li><a href=#nef-neural-engineering-network--spa-sementic-pointer-architecture>NEF (Neural Engineering Network) & SPA (Sementic Pointer Architecture)</a><ul><li><a href=#sementic-pointer>Sementic Pointer</a></li><li><a href=#embodieed-semantics>Embodieed semantics</a></li><li><a href=#working-memory>Working memory</a></li><li><a href=#spike-timing-dependent-plasticity-stdp>Spike-Timing-Dependent plasticity (STDP)</a></li></ul></li><li><a href=#spiking-models>Spiking models</a><ul><li><a href=#neural-responses>Neural responses</a></li><li><a href=#tuning-curve>Tuning curve</a></li><li><a href=#poisson-process-for-spike-firing>Poisson process for spike firing</a></li><li><a href=#rate-code-vs-temporal-code>Rate code v.s. temporal code</a></li></ul></li><li><a href=#encoding--decoding>Encoding / decoding</a></li></ul></li><li><a href=#neural-physiology>Neural Physiology</a><ul><li><a href=#excitable-membrane>Excitable membrane</a></li><li><a href=#action-potential>Action potential</a></li><li><a href=#neurotransmitters>Neurotransmitters</a></li></ul></li><li><a href=#neural-models>Neural models</a><ul><li><a href=#electrical-activity-of-neurons>Electrical activity of neurons</a></li><li><a href=#hh-model>HH model</a></li><li><a href=#considerations>Considerations</a></li></ul></li><li><a href=#dynamic-system-theory>Dynamic system theory</a><ul><li><a href=#morris-lecar-neuron-model>Morris-Lecar neuron model</a><ul><li><a href=#phase-plane-analysis>Phase plane analysis</a></li></ul></li><li><a href=#integrate-and-fire-if-model>Integrate and fire (IF) model</a></li><li><a href=#izhikevich-model>Izhikevich model</a></li><li><a href=#compartment-model>Compartment model</a></li></ul></li><li><a href=#filters>Filters</a></li><li><a href=#synapse-model>Synapse model</a></li><li><a href=#intro-to-brain>Intro to brain</a><ul><li><a href=#prerequisite>Prerequisite</a></li><li><a href=#reverse-enginering-the-brain>Reverse enginering the brain</a></li><li><a href=#why-a-brain>Why a brain</a></li><li><a href=#design-constraints>Design constraints</a></li><li><a href=#evolution-of-the-brain-in-cordates>Evolution of the brain in Cordates</a></li><li><a href=#central-pattern-generator>Central pattern generator</a></li></ul></li><li><a href=#nengo-programming>nengo programming</a><ul><li><a href=#classes>Classes</a></li><li><a href=#integrator-implementation>Integrator implementation</a></li><li><a href=#oscillator-implementation>Oscillator implementation</a></li></ul></li><li><a href=#connectivity-analysis>Connectivity analysis</a><ul><li><a href=#microscale-vs-macroscale>Microscale vs Macroscale</a></li><li><a href=#graph-theory>Graph theory</a></li><li><a href=#types-of-networks>Types of networks</a><ul><li><a href=#random>Random</a></li><li><a href=#scale-free>Scale-free</a></li><li><a href=#regular>Regular</a></li><li><a href=#modular>Modular</a></li><li><a href=#small-world>Small world</a></li></ul></li></ul></li><li><a href=#neural-engineering-framework-nef>Neural Engineering Framework (NEF)</a><ul><li><a href=#central-problems>Central problems</a></li><li><a href=#heterogeneity-in-realistic-neurla-networks>Heterogeneity in realistic neurla networks</a></li><li><a href=#building-nef-models-with-nengo>Building NEF models with nengo</a></li><li><a href=#central-nef-principles>Central NEF principles</a><ul><li><a href=#representation>Representation</a></li><li><a href=#transformation-of-encoding-information-by-neuron-clusters>Transformation of encoding information by neuron clusters</a></li><li><a href=#neual-dynamics-for-an-ensemble-of-neurons>Neual dynamics for an ensemble of neurons</a></li><li><a href=#ps>PS</a></li></ul></li></ul></li><li><a href=#neuro-representation>Neuro representation</a><ul><li><a href=#encoding--decoding-1>Encoding / decoding</a></li><li><a href=#symbols-used-when-neural-coding>Symbols used when neural coding</a></li><li><a href=#populational-encoding>Populational encoding</a><ul><li><a href=#some-linear-algebra>Some linear algebra</a></li><li><a href=#optimal-ensemble-linear-encoder>Optimal ensemble linear encoder</a></li></ul></li><li><a href=#example-horizontal-eye-position-in-nef>Example: horizontal eye position in NEF</a></li><li><a href=#vector-encoding--decoding>Vector encoding / decoding</a></li></ul></li><li><a href=#nengo-examples>Nengo examples</a></li><li><a href=#neural-transformation>Neural transformation</a><ul><li><a href=#multiplication>Multiplication</a></li><li><a href=#communication-channel>Communication channel</a></li><li><a href=#static-gain-c-multiplication-with-a-scalar>Static gain <code>c</code> (multiplication with a scalar)</a></li><li><a href=#addition>Addition</a></li><li><a href=#nonlinear-transformation>Nonlinear transformation</a></li><li><a href=#negative-weight>Negative weight</a></li></ul></li><li><a href=#neural-dynamics>Neural dynamics</a><ul><li><a href=#representation-1>Representation</a></li><li><a href=#linear-control-theory>Linear control theory</a></li><li><a href=#frequency-response-and-stability-analysis>Frequency response and stability analysis</a></li><li><a href=#neural-population-model>Neural population model</a></li><li><a href=#recurrent-connections>Recurrent connections</a></li><li><a href=#equations-for-different-levels>Equations for different levels</a></li></ul></li><li><a href=#sensation-and-perception>Sensation and Perception</a><ul><li><a href=#perception>Perception</a></li><li><a href=#psychophysica>Psychophysica</a></li></ul></li><li><a href=#vision>Vision</a><ul><li><a href=#v1-primary-visual-cortex>V1: primary visual cortex</a></li><li><a href=#successively-richer-layers>Successively richer layers</a></li><li><a href=#ventral-track>Ventral track</a></li><li><a href=#dorsal-track>Dorsal track</a></li><li><a href=#ambiguous-figures--optical-illusions>Ambiguous figures / optical illusions</a></li><li><a href=#feedback>Feedback</a></li><li><a href=#object-perception>Object perception</a></li></ul></li><li><a href=#autoencoders>Autoencoders</a><ul><li><a href=#ewerts-central-problems>Ewert&rsquo;s central problems</a></li><li><a href=#autoencoder-in-traditional-anns>Autoencoder in traditional ANNs</a></li><li><a href=#basic-machine-learning>Basic machine learning</a></li><li><a href=#classical-cognitive-systems-expert-system>Classical cognitive systems (expert system)</a></li><li><a href=#semantic-pointer-and-spa>Semantic pointer and SPA</a></li><li><a href=#encoding-information-in-the-semantic-pointer>Encoding information in the semantic pointer</a></li></ul></li><li><a href=#action-control>Action control</a><ul><li><a href=#affordance-competition-hypothesis>Affordance competition hypothesis</a></li><li><a href=#neural-optimal-control-hierachy-noch>Neural optimal control hierachy (NOCH)</a></li><li><a href=#performing-movement-in-robot-arms>Performing movement in robot arms</a></li><li><a href=#functional-level-model>Functional level model</a></li><li><a href=#rules-for-manipulation>Rules for manipulation</a></li><li><a href=#attention>Attention</a></li><li><a href=#tower-of-hanoi-task>Tower of Hanoi task</a></li><li><a href=#act-r-architecture>ACT-R architecture</a></li></ul></li><li><a href=#learning-and-memory>Learning and memory</a><ul><li><a href=#learning-in-biology>Learning in biology</a></li><li><a href=#machine-learning>Machine learning</a></li><li><a href=#biological-memories-in-detail>Biological memories in detail</a></li><li><a href=#conditioning>Conditioning</a><ul><li><a href=#terms>Terms</a></li><li><a href=#hippocampus>Hippocampus</a></li><li><a href=#inside-ltp--ltd>Inside LTP / LTD</a></li></ul></li><li><a href=#learning-rules>Learning rules</a><ul><li><a href=#hebbian>Hebbian</a></li><li><a href=#stdp>STDP</a></li><li><a href=#hpes-rule>hPES rule</a></li></ul></li><li><a href=#reinforcement-learning>Reinforcement learning</a><ul><li><a href=#value>Value</a></li><li><a href=#value-function-vs-and-prediction-error>Value function V(s) and prediction error</a></li><li><a href=#biological-rl>Biological RL</a></li><li><a href=#decision-making>Decision making</a></li></ul></li></ul></li><li><a href=#spaun-model>SPAUN model</a></li></ul></nav></div></div><div class=content id=content><p>Course Notes of Computational Cognitive Neuroscience by Prof. 鄭士康.</p><h2 id=course-information>Course Information</h2><ul><li>Lecturer: 鄭士康</li><li>Time: Fri. 789</li><li>Location: EE2-146</li><li>Homeworks:<ul><li>HW1: 10/25 (Topic free)</li><li>HW2: 11/29</li><li>Group presentation: 1/22</li></ul></li><li>Handouts (G suite): <a href=https://drive.google.com/drive/u/1/folders/1GOcrwV9lR5Xk4bqPCXhxLc600DXRYl4K>https://drive.google.com/drive/u/1/folders/1GOcrwV9lR5Xk4bqPCXhxLc600DXRYl4K</a></li></ul><h2 id=central-questions>Central Questions</h2><ul><li>Could machineㄋ perceive and think like humans?</li><li>Turing test</li><li>Stimuli -> acquire -> store -> transform (process, emotion) -> recall -> response (actions)</li></ul><h3 id=cognitive-psychology>Cognitive Psychology</h3><ul><li>Assumption: materialism: mind = brain function</li><li>Later became Cognitive Neuroscience</li><li>Models: Box and arrow -> Computational (mechanistic) vs Statistical model<ul><li>Neuronal network <em>connections</em></li></ul></li></ul><h3 id=artificial-intelligence>Artificial intelligence</h3><ul><li>Reductionism</li><li>Search space of parameters</li><li>General probelm solver</li><li>Expert systems (symbol and rule-based)<ul><li>Symbol processing ≢intellegence (Chinese room argument)</li><li>Does the machine really know semantics from the symbols and rules?</li></ul></li><li>Mimicking biological neuronetworks (H&H neuron model) -> spiking neuron network & Hebbian learning</li><li>Perceptron : Limitations by Minsky (unable to solve XOR probelm) -> 1st winter of AI</li><li>Multilayer and backpropagation: connectionism<ul><li>Parallel distributed processing (1986): actually neuronetworks (a <em>taboo</em> by then)</li></ul></li><li>Convolutional neuronal networks (CNNs)<ul><li>Computer vision</li><li>Simlimar to image processing in the visual cortex</li><li>Decomposition of features: stripes, angles, colors, etc.</li><li>Does intelligence <em>emerge</em> from complex networks?</li></ul></li><li>Dynamicism<ul><li>embodied approach</li><li>Feedback system</li><li>Systems of non-linear DEs</li></ul></li><li>Cybernetics: control system for ML (system identification)</li><li>Bayeian approach : pure statistics regradless of underlying mechanism</li></ul><h3 id=biological-plausibility>Biological plausibility</h3><ul><li>Low = little similarity to biological counterpart<ul><li>e.g. expert systems</li></ul></li><li>CNN: medium BP</li><li>SpiNNator and Nengo: high BP</li></ul><h3 id=levels-scales-of-nervous-system>Levels (scales) of nervous system</h3><ul><li>Focused on mesoscopic scale (neurons and synapses) in this course</li></ul><h2 id=builidng-a-brain-with-math-models>Builidng a brain with math models</h2><p>Why?</p><blockquote><p>Feymann: What I canot create, I do not understand.</p><ol><li>Understanding brain functions -> health (AD, PD, HD)</li><li>AI modeling and applications</li></ol></blockquote><h3 id=3d-brain-structure>3D brain structure</h3><p><a href=http://www.g2conline.org>www.g2conline.org</a></p><h3 id=the-scale-of-brain-models>The scale of brain models</h3><ul><li>Neuron</li><li>Small clusters of neurons</li><li>Large scale connections (connectomes)</li></ul><h3 id=neuron-biology>Neuron biology</h3><ul><li>dendrite</li><li>soma</li><li>axon and myelin sheath</li></ul><h3 id=hodgkin-and-huxley-model-1952>Hodgkin and Huxley model (1952)</h3><ul><li>Math model from recordings of squid giant axon</li><li>Action potential</li><li>Biophysically accurate, but harder to do numerical analysis</li><li>Chance and Design by Alan Hodgkin</li></ul><h4 id=derived-models>Derived models</h4><ul><li>Simpler models with action potentials and multiple inputs</li><li>Leaky, Integrated and Fire model (<strong>LIF</strong> model)</li><li>LEBRA: single equation for a neuron, no spatial components</li><li>Compartment model of dendrite, soma, and axon.<ul><li>Delay effect (+)</li><li>Discretization of the partial differential eqiuation (PDE) model</li><li>Could <strong>Delayed Differential Eqautions (DDEs)</strong> used in this context?</li></ul></li><li>Data (from fMIR, DTI, &mldr;) rich and theory poor</li><li>Large-scale models (connectome)</li><li>Neuromorphic hardware</li></ul><h3 id=nef-neural-engineering-network--spa-sementic-pointer-architecture>NEF (Neural Engineering Network) & SPA (Sementic Pointer Architecture)</h3><h4 id=sementic-pointer>Sementic Pointer</h4><ul><li><p>Sementics important for both symbolic and NN models</p></li><li><p>Example : autoencoder</p><ul><li>Dimemsion reduction layer by layer (raw data -> symbols)</li><li>Similar to visual cortex and associative areas</li><li>Reverse the network to adjust the weights</li><li>Loss = predicted - input</li></ul></li><li><p><strong>Spaun model</strong>: Autoencoders to preocess multiple sensory inputs as well as motor functions and decision making (transformation, working memory, reward, selection).</p></li><li><p>Ewert&rsquo;s Question: How is neural activity coordinated, learned and controlled?</p><ul><li>Capturing semantics</li><li>Encoding syntactic structures</li><li>Controlling information flow?</li><li>Memory, learning?</li></ul></li></ul><h4 id=embodieed-semantics>Embodieed semantics</h4><ul><li>Neural <em>firing patterns</em></li><li>High dimensional vector symbolic architectures</li></ul><h4 id=working-memory>Working memory</h4><ul><li>7 +/- 2 items, with highest recall for the 1st and the last item</li></ul><h4 id=spike-timing-dependent-plasticity-stdp>Spike-Timing-Dependent plasticity (STDP)</h4><ul><li>non-linear mapping for <em>learning</em> through synapses</li></ul><h3 id=spiking-models>Spiking models</h3><ul><li>Keywords: <em>spike firing rate</em>, <em>tuning curves</em>, *Poisson models</li><li>Adrian&rsquo;s frog leg test: loading induced spikes in the sciatic nerve<ol><li>Stereotyped signals = spikes</li><li>Firing rate is a function to stimuli</li><li>Fatigue (adaptation) over time</li></ol></li></ul><h4 id=neural-responses>Neural responses</h4><ul><li><p>Raster plot: dot = one spike. x: time; y: neuron id</p></li><li><p>Firing rate histogram: x: time; y: # of spikes</p></li><li><p>Neural signal response: with Dirac delta function (signal processing?)</p><p>$$
\rho(t) = \Sigma_{n=1}^N\delta(t - t_i)
$$</p></li><li><p>Indivisual spikes -> Firing rates (in Hz) with a windows (moving average)</p></li><li><p>Similar to pulse density modulation (PDM)</p></li></ul><h4 id=tuning-curve>Tuning curve</h4><ul><li>x: stimuli trait; y: response</li><li>e.g. visual cortical neuron response to line orientation</li><li>Present in both sensory and motor cortices</li></ul><h4 id=poisson-process-for-spike-firing>Poisson process for spike firing</h4><ul><li>Poisson process: a random process with constant rate (or average waiting time).</li><li>The probability <code>P</code> with <code>n</code> events fired in a period <code>T</code> given a firing rate <code>r</code> could be expressed by:</li></ul><p>$$
P_T[n] = \frac{(rT)^n}{n!}e^{-rT}
$$</p><h4 id=rate-code-vs-temporal-code>Rate code v.s. temporal code</h4><ul><li>Dense firing for the former, sparse firing for the latter</li><li>Population code (a group of neurons firing)</li></ul><h3 id=encoding--decoding>Encoding / decoding</h3><ul><li>encoding: stimuli $x(t)$ -> spikes $\delta (t-t_i)$</li><li>decoding: spikes $\delta (t-t_i)$ -> intepretation of stimuli $\hat x(t)$</li></ul><h2 id=neural-physiology>Neural Physiology</h2><ul><li>Neuron: dendrites, soma, axon</li><li>Synapses: neurotransmitter / electrical conduction<ul><li>AP from axon => Graded potential in dendrite / soma</li><li>Temporal / spatial summation of graded potential: AP in axial hillock</li></ul></li></ul><h3 id=excitable-membrane>Excitable membrane</h3><ul><li>Phospholipid bilayer (plasma membrane) as barrier</li><li>Integral / peripheral proteins: ion carriers and channels</li><li>Selected permeability to ions: Na / K gradients</li></ul><h3 id=action-potential>Action potential</h3><ul><li>Voltage-gated Na channel: both positive and negative feedback (fast)</li><li>Voltage-gated K channel: negative feedback (slow)</li><li>Leaky chloride channel: helping maintaining resting potential (constant)</li><li>Refractory period (5 ms): avaialble Na fraction is too low for AP</li><li>Nodes of Ranvier and myelin sheath: accelerates AP conduction</li></ul><h3 id=neurotransmitters>Neurotransmitters</h3><ul><li>Signaling molecules in the synaptic cleft</li><li>AP -> Ca influx -> vesicle release -> receptor bindin -> graded potentials (EPSP/IPSP) -> recycle / degradation of neurotransmitters</li></ul><h2 id=neural-models>Neural models</h2><ul><li>Features to reproduce: Integrating input, AP spikes, refractory period</li></ul><h3 id=electrical-activity-of-neurons>Electrical activity of neurons</h3><ul><li><a href=https://en.wikipedia.org/wiki/Nernst_equation target=_blank rel="noopener noreffer">Nernst equation</a> for one species of ion across a semipermeable membrane</li><li><a href=https://en.wikipedia.org/wiki/Goldman_equation target=_blank rel="noopener noreffer">GHK voltage equation</a> for multiple ions</li><li>Quasi-ohmic assumption for ion channels $I_x = g_x (V_m-E_x)$</li><li>Membrane as capacitor (1 $\mu F/ cm^2$)</li><li>Equivalent circuit: An RC circuit</li></ul><h3 id=hh-model>HH model</h3><ul><li>GHK voltage equation not applicable (not in steady state)</li><li>Using Kirchhoff&rsquo;s current law to get voltage change over time</li><li>Parameters from experiments on the squid giant axon</li><li>K channel: gating variable n</li></ul><p>$$
\begin{aligned}
g_K &= \bar g_Kn^4 \cr
\frac{dn}{dt} &= \alpha - n (\alpha + \beta)
\end{aligned}
$$</p><pre><code>α and β are determined by voltage (membrane potential)
</code></pre><ul><li><p>Na channel: two gating variables, m and h</p><p>$$
\begin{aligned}
g_{Na} &= \bar g_{Na} m^3h \cr
\frac{dm}{dt} &= \alpha_m - n (\alpha_m + \beta_m) \cr
\frac{dh}{dt} &= \alpha_h - n (\alpha_h + \beta_h) \cr
\end{aligned}
$$</p><p><code>αs and βs are determined by voltage (membrane potential)</code></p></li></ul><h3 id=considerations>Considerations</h3><ul><li>Model fidelity (biological relevance) vs simplicity (ease to simulate and analyze)</li><li>Biological plausibility</li></ul><h2 id=dynamic-system-theory>Dynamic system theory</h2><p>A system of ODEs</p><p>e.g. Butterfly effect (chaos system): small deviation of initial conditions > huge different results</p><h3 id=morris-lecar-neuron-model>Morris-Lecar neuron model</h3><ul><li>Similar to the HH model (KCL)</li><li>Ca, K, and Cl ions</li><li>two state variables: voltage (V) and one variable (w) for K</li><li>using tanh and cosh functions</li></ul><h4 id=phase-plane-analysis>Phase plane analysis</h4><ul><li>Stability: Eigenvalues of rhs Jacobian matrix in the steady-state</li><li>External current (Ie) = 0: single stable steady-state (interscetion of V and w nullclines)</li><li>Increasing Ie: shifting V null cline => unstable steady-state (limit cycle)</li><li>Bifurcation: V vs Ie</li></ul><h3 id=integrate-and-fire-if-model>Integrate and fire (IF) model</h3><ul><li>A simple RC circuit</li><li>Single state variable (V)</li><li>Use of conditional statements to control spiking firing and refractory period</li><li>Used in nengo (plus leaky = LIF model)</li><li>Firing rate adaption: IF model + more terms</li></ul><h3 id=izhikevich-model>Izhikevich model</h3><ul><li>Two state variables</li><li>Realistic spike patterns by adjusting parameters</li><li>Could be used in large systems (100T synapses)</li></ul><h3 id=compartment-model>Compartment model</h3><ul><li>Spatial discretization for neuron models</li><li>Coupled RC circuits -> FEM grids</li></ul><h2 id=filters>Filters</h2><ul><li>Presynaptic AP -> synapse neurotransmitter release -> Postsynaptic potentials</li><li>Approximated by an LTI(linear, time invariant) system</li><li>Linear: superposition</li><li>Time invariant: unchanged with time shifting</li><li>Impulse response: given a impulse (delta function) -> h(t), transformed results</li><li>Convolution: h(t) instead of the system itself</li><li>Fourier transform: Convolution -> multiplication</li></ul><h2 id=synapse-model>Synapse model</h2><ul><li>Synapse = RC low pass filters with time scale = $\tau$</li><li>$\tau$ is dependent on types of neurotransmittera and receptors</li></ul><h2 id=intro-to-brain>Intro to brain</h2><h3 id=prerequisite>Prerequisite</h3><ul><li>Simple linear algebra (vector and matrix operations)</li><li>Graph theory: connections</li></ul><h3 id=reverse-enginering-the-brain>Reverse enginering the brain</h3><ul><li>engineeringchallenges.org</li><li>Complexity, scale, connection, plasticiy, low-power</li><li>Design: brain scheme; designer: natural selection</li></ul><h3 id=why-a-brain>Why a brain</h3><ul><li>To survive and thrive.</li><li>Brainless (single-celled organisms): simple preceptions and reactions. Some endogenous activity</li><li>Simple brain (C. elegans): aversive response and body movement<ul><li>Connectome routing study (as in EDA) showed 90% of the neurons are in the optimal positions</li></ul></li><li>General scheme: sensory -> CNS -> motor (with endogenous states (thoughts) in the CNS)</li></ul><h3 id=design-constraints>Design constraints</h3><ul><li>Information theory (information efficiency)</li><li>Energy efficiency</li><li>Space efficiency</li><li>Human brain is already relatively larger than almost all animals</li></ul><h3 id=evolution-of-the-brain-in-cordates>Evolution of the brain in Cordates</h3><ul><li>Dorsal neural tube -> differentialtion respecting sensory,motor, and inter connections</li></ul><h3 id=central-pattern-generator>Central pattern generator</h3><ul><li>The brainless walking cat: endogenous activity in the spinal cord</li><li>Main functioN unit in the CNS</li></ul><h2 id=nengo-programming>nengo programming</h2><h3 id=classes>Classes</h3><ul><li>Network: model itself</li><li>Node: input signal</li><li>Ensemble: neuronss</li><li>Coonnection: synapses</li><li>Probe: output</li><li>Simulator: simulator (literally)</li></ul><h3 id=integrator-implementation>Integrator implementation</h3><ul><li>Similar to the Euler method in numerical integration</li></ul><p>$$
y[n] = A { y[n-1] + \Delta t x[n-1] }
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>nengofrom</span> <span class=n>nengo</span><span class=o>.</span><span class=n>processes</span>
<span class=kn>import</span> <span class=nn>Piecewise</span>

<span class=c1># The model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Network</span><span class=p>(</span><span class=n>label</span><span class=o>=</span><span class=s1>&#39;Integrator&#39;</span><span class=p>)</span>

<span class=k>with</span> <span class=n>model</span><span class=p>:</span>
    <span class=c1># Neurons representing one number</span>
    <span class=n>A</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Ensemble</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>dimensions</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

    <span class=c1># Input signal</span>
    <span class=n>src</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Node</span><span class=p>(</span><span class=n>Piecewise</span><span class=p>({</span><span class=mi>0</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span> <span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>4</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span><span class=mi>5</span><span class=p>:</span> <span class=mi>0</span><span class=p>}))</span>

    <span class=n>tau</span> <span class=o>=</span> <span class=mf>0.1</span>

    <span class=c1># Connect the population to itself</span>
    <span class=c1># transform: transformation matrix</span>
    <span class=c1># synapse: time scale of low pass filter</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=p>[[</span><span class=mi>1</span><span class=p>]],</span> <span class=n>synapse</span><span class=o>=</span><span class=n>tau</span><span class=p>)</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=p>[[</span><span class=n>tau</span><span class=p>]],</span> <span class=n>synapse</span><span class=o>=</span><span class=n>tau</span><span class=p>)</span>
    <span class=n>input_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
    <span class=n>A_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>synapse</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># Create our simulator</span>
<span class=k>with</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Simulator</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=k>as</span> <span class=n>sim</span><span class=p>:</span>
    <span class=c1># Run it for 6 seconds</span>
    <span class=n>sim</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=mi>6</span><span class=p>)</span>
<span class=c1># Plot the decoded output of the ensemble</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sim</span><span class=o>.</span><span class=n>trange</span><span class=p>(),</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>input_probe</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Input&#34;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sim</span><span class=o>.</span><span class=n>trange</span><span class=p>(),</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>A_probe</span><span class=p>],</span> <span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Integrator output&#34;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>();</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></td></tr></table></div></div><h3 id=oscillator-implementation>Oscillator implementation</h3><p>Harmonic oscillator: one 2nd order ODE -> two 1st order ODEs</p><p>$$
\begin{aligned}
\frac{d^2x}{dt^2} &= -\omega^2 x \cr
\vec{x} &= \begin{bmatrix}x \cr \frac{dx}{dt} \end{bmatrix} \cr
\frac{d\vec{x}}{dt} &= \begin{bmatrix}0 & 1 \cr -\omega^2 & 1 \end{bmatrix} \vec{x} = A \vec{x}
\end{aligned}
$$</p><p><strong>nengo</strong>:</p><p>$$
\begin{aligned}
\vec{x} &= \begin{bmatrix}x_0 \cr x_1 \end{bmatrix} \cr
\vec{x}[n] &= \begin{bmatrix}1 & \Delta t \cr-\omega^2\Delta t & 1 \end{bmatrix} \cr \vec{x}[n-1] &= B \vec{x}[n-1] \cr
\end{aligned}
$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>nengo</span>
<span class=kn>from</span> <span class=nn>nengo.processes</span> <span class=kn>import</span> <span class=n>Piecewise</span>

<span class=c1># Create the model object</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Network</span><span class=p>(</span><span class=n>label</span><span class=o>=</span><span class=s1>&#39;Oscillator&#39;</span><span class=p>)</span>

<span class=k>with</span> <span class=n>model</span><span class=p>:</span>
    <span class=c1># Neurons representing 2 numbers (dim = 2)</span>
    <span class=n>neurons</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Ensemble</span><span class=p>(</span><span class=mi>200</span><span class=p>,</span> <span class=n>dimensions</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=c1># Input signal</span>
    <span class=n>src</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Node</span><span class=p>(</span><span class=n>Piecewise</span><span class=p>({</span><span class=mi>0</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=mf>0.1</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]}))</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>neurons</span><span class=p>)</span>
    <span class=c1># Create the feedback connection. Note the transformation matrix</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>neurons</span><span class=p>,</span> <span class=n>neurons</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]],</span> <span class=n>synapse</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

    <span class=n>input_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=s1>&#39;output&#39;</span><span class=p>)</span>
    <span class=n>neuron_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>neurons</span><span class=p>,</span> <span class=s1>&#39;decoded_output&#39;</span><span class=p>,</span> <span class=n>synapse</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Create the simulator</span>
<span class=k>with</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Simulator</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=k>as</span> <span class=n>sim</span><span class=p>:</span>
    <span class=c1># Run it for 5 seconds</span>
    <span class=n>sim</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sim</span><span class=o>.</span><span class=n>trange</span><span class=p>(),</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>neuron_probe</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Time (s)&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=s1>&#39;large&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>([</span><span class=s1>&#39;$x_0$&#39;</span><span class=p>,</span> <span class=s1>&#39;$x_1$&#39;</span><span class=p>])</span>

<span class=n>data</span> <span class=o>=</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>neuron_probe</span><span class=p>]</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>data</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>data</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decoded Output&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;$x_0$&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;$x_1$&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></td></tr></table></div></div><h2 id=connectivity-analysis>Connectivity analysis</h2><ul><li>Structural: anatomical structures e.g. water diffusion via DTI</li><li>Functional: statisitc, dynamic weights</li><li>Effective: causal interactions (presynaptic spikes -> postsynamptic firing)</li><li>ref. 因果革命</li></ul><h3 id=microscale-vs-macroscale>Microscale vs Macroscale</h3><ul><li>Microscale: um ~ nm (synapses)</li><li>Macroscale: mm (voxels) coherent regions</li></ul><h3 id=graph-theory>Graph theory</h3><ul><li>Node: brain areas (or neurons)</li><li>Edges: connections (or synapses)</li><li>Represented by adjacency matrices (values = connection weights)</li></ul><h3 id=types-of-networks>Types of networks</h3><ul><li>Nodes in a circle; Connections in an adjacency matrix</li><li>Measure: degrees of a node (inward / outward) / neighborhood (Modularity Q, Small-worldness S)</li></ul><h4 id=random>Random</h4><p>Same edge probability</p><h4 id=scale-free>Scale-free</h4><ul><li>Power law</li><li>Fractal</li><li>Increased robustness to neural damage</li></ul><h4 id=regular>Regular</h4><ul><li>Local connections only</li></ul><h4 id=modular>Modular</h4><ul><li>hierarchial clusters</li><li>Built by attraction and repulson between nodes</li><li>In some biological neural networks</li></ul><h4 id=small-world>Small world</h4><ul><li>Similar to social networks, sparse global connections</li><li>A few hubs (opinion leaders) with high degrees (connecting edges)</li><li>Rich hub organization in biological neural networks (10 times the connections to the average)</li><li>Anatomical basis (maximize space / energy efficiency)</li></ul><h2 id=neural-engineering-framework-nef>Neural Engineering Framework (NEF)</h2><ul><li>By Eliasmith</li><li>Intended for constant structures without synaptic plasticity<ul><li>Compared to SNNs (with learning = synaptic plasticity)</li></ul></li><li>Nerual compiler (high level function &lt;=> low level spikes)</li></ul><h3 id=central-problems>Central problems</h3><ul><li>Stimuli detection (sensors)</li><li>Representation / manipulation of information (sensory n.)<ul><li>As spikes (pulse density modulation = PDM)</li></ul></li><li>Recall / transform (CNS)</li></ul><h3 id=heterogeneity-in-realistic-neurla-networks>Heterogeneity in realistic neurla networks</h3><ul><li>Different set of parameters for each neuron in response to stimuli</li><li>Represented as <em>tuning curves</em></li></ul><h3 id=building-nef-models-with-nengo>Building NEF models with nengo</h3><ul><li>Hypothesis / data / structure from the real counterpart</li><li>Build NEF and check behavior</li><li>Rinse and repeat</li></ul><h3 id=central-nef-principles>Central NEF principles</h3><h4 id=representation>Representation</h4><ul><li>Action potential: digital, non-linear encoding (axon hillock)</li><li>Graded potential: analog, linear decoding (dendrite)</li><li>Compared to ANNs:<ul><li>dendrite = wieghted sum from other neurons</li><li>axon hillock: non-linear activation function (real number output)</li></ul></li><li>Examples: Physical values: heat, light, velocity, position<ul><li>mimicking sensory neurons = transducer producing pulse signals</li></ul></li></ul><h4 id=transformation-of-encoding-information-by-neuron-clusters>Transformation of encoding information by neuron clusters</h4><h4 id=neual-dynamics-for-an-ensemble-of-neurons>Neual dynamics for an ensemble of neurons</h4><p>HH mdoel, LIF, control theory</p><h4 id=ps>PS</h4><ul><li>Neurons are noisy</li><li>In the NEF: the basic unit is an ensemble of neurons</li><li>Post synaptic current: approximated by one time constant</li></ul><h2 id=neuro-representation>Neuro representation</h2><h3 id=encoding--decoding-1>Encoding / decoding</h3><ul><li>Ensemble = Digital-analog converter like digital audio processing</li></ul><h3 id=symbols-used-when-neural-coding>Symbols used when neural coding</h3><ul><li>x: strength of external stimuli</li><li>J(x): x-induced current</li><li>$a(x) = G[J(x)]$: firing rate of spikes ≈ activation function in ANNs</li><li>Most important parameters<ul><li>$J_{th}$ (threshold current)</li><li>$\tau_{ref}$ (refractory period → maximal spiking rate)</li></ul></li></ul><h3 id=populational-encoding>Populational encoding</h3><p>A group of neurons determine the value by their spikes collectively.
Contrary to <em>sparse coding</em>.</p><h4 id=some-linear-algebra>Some linear algebra</h4><ul><li>Any vector couldbe decomposed as an unique linear cmobination of basis vectors</li><li>The most convienent ones are orthogonal bases e.g. sin / cos in Fourier series</li><li>The stimuli through the ensemble could be estimated from the linear combination of wieghts of neurons with different tuning curves</li><li>Simpleset : two neuron model (on and off)</li><li>Adding more and more neurons differing in tuning curves (more bases) = more accurate representation</li></ul><h4 id=optimal-ensemble-linear-encoder>Optimal ensemble linear encoder</h4><ul><li>Calculated by solving a linear system</li><li>Nengo derives the best set of weights for an ensemble of neurons automatically</li><li>Adding Gaussian noise in fact enhanced the robustness of the matrix of tuning cuves</li></ul><h3 id=example-horizontal-eye-position-in-nef>Example: horizontal eye position in NEF</h3><ul><li>System description<ul><li>Max firing rate = 300 Hz</li><li>On-off neurons</li><li>Goal: linear tuning curve</li></ul></li><li>How neurons work in abducens motor neuron: an integrator</li><li>Populations, noise, and constraints</li><li>Solution errors associated to the number of neurons<ul><li>Noise error</li><li>Static error</li><li>Rounding error</li></ul></li></ul><h3 id=vector-encoding--decoding>Vector encoding / decoding</h3><ul><li>Similar to the scalar case, but replaced with vectors</li><li>Automatically handled by the nengo framework</li></ul><h2 id=nengo-examples>Nengo examples</h2><ul><li><code>RateEncoding.py</code></li><li><code>ArmMovement2D.py</code></li></ul><h2 id=neural-transformation>Neural transformation</h2><ul><li>Linear</li><li>Non-linear</li><li>Weighting: positive (excitatory) / negative (inhibitory)</li></ul><h3 id=multiplication>Multiplication</h3><ul><li>Controlled integrator (memory)</li><li>ref: <code>Multiplication.py</code></li><li>Traditional ANN counterpart: Neural clusters A and B fully connected to combination layer, respectively</li><li>Making a subnetwork: factory function</li></ul><h3 id=communication-channel>Communication channel</h3><ul><li>Output of one ensemble => Input of another ensemble</li><li>Traditional ANN counterpart: fully-connected layers</li><li>$w_{ji} = \alpha_je_jd_i$</li><li>nengo: simply <code>Connection(A, B)</code></li></ul><h3 id=static-gain-c-multiplication-with-a-scalar>Static gain <code>c</code> (multiplication with a scalar)</h3><ul><li>$w_{ji} = c\alpha_je_jd_i$</li><li>nengo: <code>Connection(A, B, transform=c)</code></li></ul><h3 id=addition>Addition</h3><ul><li>c = a + b</li><li>nengo: <code>Connection(A, C); Connection(B, C)</code></li><li>Adding two vectors: just change <code>dimesion</code></li></ul><h3 id=nonlinear-transformation>Nonlinear transformation</h3><ul><li>nengo: define a vector transformation functon <code>f</code> => <code>Connection(A, B, function=f)</code></li></ul><h3 id=negative-weight>Negative weight</h3><ul><li>An ensemble of inhibitory neurons</li></ul><h2 id=neural-dynamics>Neural dynamics</h2><ul><li>Neural control systems: non-linear, time-variant (modern control theory)</li></ul><h3 id=representation-1>Representation</h3><ul><li>1st order ODEs</li><li>State variables as a vector</li><li>$\mathbf{x}(t) = \mathbf{x}(t - \Delta t) + f(t - \Delta t, \mathbf{x}(t - \Delta t))$</li><li>Example: cellular automata finite state machine (Game of life)</li></ul><h3 id=linear-control-theory>Linear control theory</h3><p>u: input, y: output, x: internal states
$\mathbf{\dot{x}}(t) = A \mathbf{\dot{x}}(t) + B \mathbf{u}(t)$
$\mathbf{y}(t) = C \mathbf{x}(t) + D \mathbf{u}(t)$</p><h3 id=frequency-response-and-stability-analysis>Frequency response and stability analysis</h3><ul><li>Laplace transform $L{f(t)} = \int^\infty_0e^{-st}f(t)dt = F(s)$</li><li>Impulse response: $h(t) = \frac{1}{\tau}e^{-t/\tau}, \ H(s) = \frac{1}{1 + s\tau}$. Stable (pole at the left half plane)</li><li>Convolution in the time domain = multiplication in the Laplace (s-domain)</li></ul><h3 id=neural-population-model>Neural population model</h3><ul><li>Linear decoder for post-synaptic current (PSC)<ul><li>$A^\prime = \tau A + I$</li><li>$B^\prime = \tau B$</li></ul></li></ul><h3 id=recurrent-connections>Recurrent connections</h3><ul><li>Positive feedback: <code>Feedback1.py</code></li><li>Negative feedback: <code>Feedback2.py</code> (without stimuli), <code>Feedback3.py</code> (with stimuli)</li><li>Dynamics: <code>Dynamics1.py</code> and <code>Dynamics2.py</code>: step stimuli + feedback</li><li>Integrators: $A = \frac{-1}{\tau} I$</li><li>Oscillators: $A = \begin{bmatrix} 0&1 cr\ -\omega^2&0 \cr \end{bmatrix}$</li></ul><h3 id=equations-for-different-levels>Equations for different levels</h3><ul><li>Nengo: higher level</li><li>Implementation: lower rate / spiking levels</li></ul><h2 id=sensation-and-perception>Sensation and Perception</h2><p>Environment (stimulation) (analog signal) -> sensory transduction (feature extraction) -> impulse signal (sensory nerve) -> perceptions (sensory cortex) -> processing (CNS) -> action selection (motor cortex) -> impulse signal (motor nerve) -> acuator(e.g. muscle) -> action</p><h3 id=perception>Perception</h3><ul><li>Internal representation of stimuli impulses</li><li>The experience in the association cortex (not necessary the same as the outside world)</li><li>Book: making uo the mind</li></ul><h3 id=psychophysica>Psychophysica</h3><p>e.g. Psychoacoustics: used in MP3 compression</p><ul><li>Threshold in quiet / noisy environment</li><li>Equal-loudness contour in different frequencies</li><li>Weber&rsquo;s law: change perceived in percent change $S = klg\frac{I}{I_0}$</li></ul><h2 id=vision>Vision</h2><ul><li>Convergence of information inside retina<ul><li>260M photoreceptor cells indirectly connected to 2M ganglion (optic nerve) cells)</li><li>Dimension reduction (pooling / convolution)</li></ul></li><li>Need of learning to see (mechanism of amblyopia): Neural wiring in the visual tract and the visual cortex (training of CNNs)</li></ul><h3 id=v1-primary-visual-cortex>V1: primary visual cortex</h3><ul><li>Detection of oriented edges, grouped by cortical columns with sensitivity to different angles</li><li>Similar to the tuning curve in NEF</li></ul><h3 id=successively-richer-layers>Successively richer layers</h3><p>Optic nerve -> LGN (thalamus) -> V1 -> V2 / V4 -> dorsal (metric) or ventral (identification) tracks</p><ul><li>Feature extraction</li><li>Similar to convolutional neural network (CNNs)<ul><li>Demonstrated in fMRI</li></ul></li></ul><h3 id=ventral-track>Ventral track</h3><ul><li><strong>What</strong> is the object?</li><li>V2 / V4 -> Post. Inf. temporal (PIT) cortex -> Ant. Inf. temporal (AIT) cortex</li><li>PIT: More complex features e.g. fusiform face area for fast facial recognition</li><li>AIT: Classification of objects regardless of size, color, viewing angle&mldr;<ul><li>Hyperdimensional vector (EECS) = semantic pointer (NEF)</li><li>Neural emsemble of 20000 in monkeys</li></ul></li><li>Thus the functions of the temporal lobe = categorizing the world:<ul><li>Primary and associative auditory</li><li>Labeling visual objects</li><li>Language processing for both visual and auditory cues</li><li>Episodic memory formation by hippocampus</li></ul></li></ul><h3 id=dorsal-track>Dorsal track</h3><ul><li><strong>Where</strong> is the object?</li><li>V1 -> V2 -> V5 -> parietal lobe (visual association area)</li><li>metrical information and mathematics</li><li>Motion detection and information for further actions</li></ul><h3 id=ambiguous-figures--optical-illusions>Ambiguous figures / optical illusions</h3><p>Forms 2 attractors (intepretations)</p><p>e.g Necker cube</p><h3 id=feedback>Feedback</h3><ul><li>External cue and expectation (top down perception)</li><li>Report to LGN about the error</li></ul><h3 id=object-perception>Object perception</h3><ul><li>In biology: robust recognition despite color, viewing angle differences (object consistency)</li><li>View-dependent frame of reference vs. View-invariant (grammer pattern) frame of reference</li></ul><h2 id=autoencoders>Autoencoders</h2><h3 id=ewerts-central-problems>Ewert&rsquo;s central problems</h3><ul><li>Preception: encoding stimuli from analog to digital spikes</li><li>Central processing: transformation and recall of information, action selection</li><li>Action execution: decoding digital spikes to response</li></ul><h3 id=autoencoder-in-traditional-anns>Autoencoder in traditional ANNs</h3><ul><li>Compressing the input into a smaller (dim.) representation then expand to the estimation<ul><li>Hyper dimension vector in CS</li><li>Semantic pointer in NEF</li></ul></li><li>Novelty detection: comparison of the input to the output from trained autoencoder</li></ul><h3 id=basic-machine-learning>Basic machine learning</h3><ul><li>For y = f(x), find f</li><li>Training, testing, validation sets</li><li>Learning curves: overfitting if overtraining</li><li>Cross validation to reduce overfitting and increase testing accuracy<ul><li>K-fold cross validation</li></ul></li><li>SVM: once worked better than ANNs<ul><li>Converting low dim but complex border to higer dim. simpler (even linear) border by trasnformation of data points</li></ul></li></ul><h3 id=classical-cognitive-systems-expert-system>Classical cognitive systems (expert system)</h3><ul><li>Symbols and syntax processing (LISP)</li><li>Failed due to low BP (unable to solve to meaning of symbols)</li><li>Another attempt: connectionist (semantic space) => too complex</li><li>Symbol binding system: 500M neurons to recognize simple sentences (fail)</li><li>Until the semantic pointer hypothesis: explaining high level cognitive function<ul><li>Halle Berry neurons (grandmother neurons): highly selective to one category instances (sparse coding)</li><li>However most instances are population coding</li></ul></li></ul><h3 id=semantic-pointer-and-spa>Semantic pointer and SPA</h3><ul><li>Equals to hyperdimensional vector in the mathematical sense</li><li>Presented by an ensemble of neurons in biology</li><li>The semantic space (hyperdimensional space) holds information features<ul><li>Needs enough dimesions for the overwhelming number of concepts in the world</li></ul></li><li>Pointers = symbols = general concepts<ul><li>Indirect addressing of complex information</li><li>Shallow and deep manipulation (dual coding theory)</li><li>Efficient transformation (call by address)</li></ul></li><li>Shallow semantics (e.g. text mining): symbols and stats only, does not encode the meaning of words</li><li>Nengo: <code>nengo-spa</code></li></ul><h3 id=encoding-information-in-the-semantic-pointer>Encoding information in the semantic pointer</h3><p>Circular convolution for syntax processing</p><ul><li>Readily extract the information in SP after filtered some noise</li><li>Does not incur extra dimensions</li><li>Works on reals numbers (XOR works on binaries only)</li><li>Solves Jackendoff&rsquo;s challenges<ul><li>Binding problem : red + square vs green + circle</li><li>Problem of 2: small star vs big star</li><li>Problem of variable: blue fly (n.) vs. blue fly(v.): binding restrictions</li><li>Binding in working memory vs long-term memory</li></ul></li></ul><p>One could coombine multiple sources of input (word, visual, smell, auditory)</p><h2 id=action-control>Action control</h2><p>Behavioral pattern / coordination</p><h3 id=affordance-competition-hypothesis>Affordance competition hypothesis</h3><ul><li>Affordance part: continously updating the status</li><li>Competition part: select best action by utility (spiking activity)
In biology:</li><li>Premotor / supplementary motor cortex<ul><li>Weighted summation of previously learned motor components (basis functions) -> desired movement</li></ul></li><li>Primary motor cortex</li><li>Basal ganglia<ul><li>Caudate, putamen, globus pallidus, SN</li><li>Excitation and inhibitory projections</li><li>Dopaminergic neurons: reward expectation: reinforcement learning</li><li>Movement initiation</li><li>Direct, indirect, and hyperdirect pathways</li></ul></li><li>Cerebellum<ul><li>Learning and control of movements</li><li>Error-driven (similar to back propagation): supervised learning</li></ul></li><li>Hippocampus: self-organizing (Hebbian, STDP): unsupervised learning</li></ul><h3 id=neural-optimal-control-hierachy-noch>Neural optimal control hierachy (NOCH)</h3><p>Computational model by students of Eliasmith, including:</p><ul><li>Cortex (premotor)</li><li>cerebellum</li><li>basal ganglia</li><li>motor cortex</li><li>brain stem and spinal cord</li></ul><h3 id=performing-movement-in-robot-arms>Performing movement in robot arms</h3><ul><li>Joint angle space [θ1, θ2, &mldr;]: degree of freedom</li><li>Operational space (end point vector)</li></ul><p>High level -> mid level -> low level control signals</p><p>Similar to the latter half of autoencoder.</p><h3 id=functional-level-model>Functional level model</h3><p>Loop of</p><ul><li>Cortex: memory / transformations, crude selection</li><li>Basal ganglia: utility -> action (cosine similarity)</li><li>Thalamus: monitoring</li></ul><h3 id=rules-for-manipulation>Rules for manipulation</h3><ul><li>Symbols, fuzzy logic, but not compatible to neural networks</li><li>Basal ganglia: manipulation
$$
\vec{s} = M_b \cdot \vec{w}
$$</li><li>Rehearsal of alphabet <code>sequence.py</code></li></ul><h3 id=attention>Attention</h3><p>Timing of neuron&rsquo;s response: ~15ms delay to make decision.</p><p>The less utility difference, the longer the latency.</p><ul><li>Parametric study on computational models</li></ul><h3 id=tower-of-hanoi-task>Tower of Hanoi task</h3><ul><li>Perceptural strategy from symbolic calculation is not biologically plausible in Eliasmith paper (not learning the rule).</li><li>150k neurons</li></ul><h3 id=act-r-architecture>ACT-R architecture</h3><p>Symbol -> neural networks</p><p>Comparative to fMRI BOLD signal.</p><h2 id=learning-and-memory>Learning and memory</h2><p>Ref: Neuroeconomics, declision making and the brain.</p><p>Learning: stimulus altered behavior. Not hardwired.</p><p>Memory: storage of learned information.</p><h3 id=learning-in-biology>Learning in biology</h3><ul><li>Neural level: synapse strength, neural gene expression</li><li>Brain regions: coordination</li></ul><h3 id=machine-learning>Machine learning</h3><ul><li>Weight changes in synaptic connections</li><li>Neural activity states: dynamic stability (attractor)</li></ul><h3 id=biological-memories-in-detail>Biological memories in detail</h3><ul><li>Declarative (explicit) memory: medial temporal lobe and neocortex<ul><li>Events (episodic): 5W1H, past experience</li><li>Facts (semantic): grammar, common sense (context-free)</li></ul></li><li>Non-declarative memory<ul><li>Procedual: basal ganglia</li><li>Perceptual priming: short path for recall for previous stimuli</li><li>Conditioning: cerebellum</li><li>Non-associative: reflex</li></ul></li><li>Sensory memory: buffer<ul><li>9-10 sec for schoic (hearing)</li><li>0.5 sec for iconic (vision)</li></ul></li></ul><h3 id=conditioning>Conditioning</h3><ul><li>Pavlov&rsquo;s dog: classical conditioning</li><li>Skinner: operant conditioning</li><li>Acquisition, extinction, spontaneous recovery (long-term memory)</li></ul><h4 id=terms>Terms</h4><ul><li>Memory: recall / recognize past experience</li><li>Conditioning: associate event and response</li><li>Learning: change behavior to stimuli</li><li>Plasticity: change neural connections<ul><li>Functional: chemical connection change</li><li>Structural: physical connection change</li></ul></li></ul><h4 id=hippocampus>Hippocampus</h4><p>Dentate gyrus -> CA3 -> CA1</p><ul><li>Long-term potentiation (LTP) upon high freq stimulation: enhances EPSP</li><li>Long-term depression (LTD) upon los freq stimulation: inhibits EPSP</li><li>Neural growth even at 40 y/o</li></ul><h4 id=inside-ltp--ltd>Inside LTP / LTD</h4><p>Neurotransmitters</p><ul><li>Glutamate (AMPAR, NMDAR) : excitary</li><li>GABA: inhibitory</li></ul><p>Second messengers (mid-term effcts)</p><h3 id=learning-rules>Learning rules</h3><h4 id=hebbian>Hebbian</h4><ul><li><p>Freud -> Hebb (1949): fire together, wire together</p><p>$
\Delta w = \epsilon\gamma_i\gamma_j
$</p><p>$\epsilon$: learning rate</p><p>$\gamma_i$: postsynaptic firing rate</p><p>$\gamma_j$: presynaptic firing rate</p></li></ul><h4 id=stdp>STDP</h4><ul><li>Spike-time-dependent plasticity from experimental data</li><li>Pre synaptic spike then post one: LTP</li><li>Post synaptic spike then pre one: LTD</li></ul><h4 id=hpes-rule>hPES rule</h4><p>Limitations on weight change</p><p>$$
\Delta w_{ij} = \alpha_ja_{j}(k_1e_jE + k_2a_i(a_j - \theta))
$$</p><h3 id=reinforcement-learning>Reinforcement learning</h3><p>E.g. operant conditioning (Skinner)</p><h4 id=value>Value</h4><ul><li>Expected value $E[ x ]$</li><li>Expected utility $U(E[ x ]) \approx log(E[ x ])$</li><li>Basic axiomatic form (Pareto)</li><li>Weak axioms of revealed perference (WARP)</li><li>Generated axioms of revealed perference (GARP)</li></ul><h4 id=value-function-vs-and-prediction-error>Value function V(s) and prediction error</h4><p>$V_{k+1}(s_k) = (1-\alpha)V_k(s_k) + \alpha\delta_k$</p><p>Error: $\delta_k = r_k - V_k(s_k)$</p><p>For multiple stimuli: Rescorla-Wagner model</p><p>$V_k^{net} = \Sigma V_{k}(stim)$</p><h4 id=biological-rl>Biological RL</h4><p>Dopamine reward pathway for movement and motivation.</p><p>Increased dopamine secretion for a sudden reward. The same as Error: $\delta_k = r_k - V_k(s_k)$</p><h4 id=decision-making>Decision making</h4><ul><li><p>Problem: no immediate ffeedback (reward) => need to think about the future and maximize aggregate reward</p></li><li><p>Bellman equation: reduction of recursive reward with temporal difference ($V_k(S_{t+1})- V_k(S_t)$)</p><p>$V(S_t) = r(S_t) + E[V(S_{t+1})|S_t]$</p><p>$\delta_t = r_t + V_k(S_{t+1})- V_k(S_t)$</p></li><li><p>Markov decision process</p></li><li><p>Q learning</p><ul><li>Q function $Q(s, \pi)$</li><li>Policy $\pi(s)$: mapping state to actions</li></ul><p>$Q_{t+1}(S_t, a_t) = Q_{t}(S_t, a_t) + \alpha\delta_t$</p><p>$\delta_t = r_t \gamma_{max}Q_{t+1}(S_t, a_t) - Q_{t}(S_t, a_t)$</p></li></ul><h2 id=spaun-model>SPAUN model</h2><p>SPAUN = Semantic pointer architecture unified network, all things put together</p><ul><li>Single perceptual system (eye)</li><li>Single motor system (arm)</li><li>Background knowledge (SPA)</li><li>Abilities<ul><li>Smiliar to human in working mem limitations (3-7)</li><li>Behavior flexibility</li><li>Adaptation to reward</li><li>Confusion to invalid input</li></ul></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2021-06-18&nbsp;<a class=git-hash href=https://github.com/sosiristseng/sosiristseng.github.io/commit/02d0bed84c96cb931b0f5ec31d121bd27b56bd39 target=_blank title="commit by Wen-Wei Tseng(sosiristseng@gmail.com) 02d0bed84c96cb931b0f5ec31d121bd27b56bd39: course notes" rel="noopener noreferrer">
<i class="fas fa-hashtag fa-fw"></i>02d0bed</a></span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/posts/course/ccns/index.md target=_blank rel="noopener noreferrer">Read Markdown</a></span></div><div class=post-info-share><span><a href=# onclick=return!1 title="Share on Twitter" data-sharer=twitter data-url=https://sosiristseng.github.io/posts/course/ccns/ data-title="Computational Cognitive Neuroscience" data-via=sosiristseng><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="Share on Facebook" data-sharer=facebook data-url=https://sosiristseng.github.io/posts/course/ccns/><i class="fab fa-facebook-square fa-fw"></i></a><a href=# onclick=return!1 title="Share on Linkedin" data-sharer=linkedin data-url=https://sosiristseng.github.io/posts/course/ccns/><i class="fab fa-linkedin fa-fw"></i></a><a href=# onclick=return!1 title="Share on Hacker News" data-sharer=hackernews data-url=https://sosiristseng.github.io/posts/course/ccns/ data-title="Computational Cognitive Neuroscience"><i class="fab fa-hacker-news fa-fw"></i></a><a href=# onclick=return!1 title="Share on Reddit" data-sharer=reddit data-url=https://sosiristseng.github.io/posts/course/ccns/><i class="fab fa-reddit fa-fw"></i></a><a href=# onclick=return!1 title="Share on Line" data-sharer=line data-url=https://sosiristseng.github.io/posts/course/ccns/ data-title="Computational Cognitive Neuroscience"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@4.23.0/icons/line.svg></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=# onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/course/academic-writing-04/ class=prev rel=prev title="Academic Writing Week 4"><i class="fas fa-angle-left fa-fw"></i>Academic Writing Week 4</a>
<a href=/posts/course/circuits/ class=next rel=next title="Applied electricity">Applied electricity<i class="fas fa-angle-right fa-fw"></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.84.0">Hugo</a> | Theme - <a href=https://github.com/HEIGE-PCloud/DoIt target=_blank rel="noopener noreffer" title="DoIt 0.2.10"><i class="far fa-edit fa-fw"></i> DoIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2020 - 2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://sosiristseng.github.io/ target=_blank rel="noopener noreferrer">Wen-Wei Tseng</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/contrib/copy-tex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.0/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.4.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.13.9/dist/contrib/mhchem.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:14},comment:{},cookieconsent:{content:{dismiss:"Got it!",link:"Learn more",message:"This website uses Cookies to improve your experience."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"No results found",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>